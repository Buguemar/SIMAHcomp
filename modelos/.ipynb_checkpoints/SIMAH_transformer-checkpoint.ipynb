{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import os, re, sys\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "import ast\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "'''\n",
    "Compatible with tensorflow backend\n",
    "gamma entre más alto más tolerante (queremos un gamma chico pero no demasiado! 0.25 - 5)\n",
    "alpha pesos por clase (weights)\n",
    "'''\n",
    "def focal_loss(gamma=2., weights=1):   #weights np.asarray()\n",
    "    weights= K.variable(weights)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.clip(y_true, K.epsilon(),1)\n",
    "        y_pred = K.clip(y_pred,K.epsilon(),1)\n",
    "        return - K.sum(weights* K.pow(1. - y_pred, gamma)* y_true * K.log(y_pred), axis=-1) \n",
    "    return focal_loss_fixed\n",
    "\n",
    "keras.losses.focal_loss=keras.losses.MSE\n",
    "keras.losses.focal_loss_fixed=keras.losses.MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nh\t\t| IH\t\t| PH\t\t| SH\t\t| F-Macro\n",
      "0.929514 \t 0.0 \t\t 0.0 \t\t 0.84362934 \t 0.443285836068\n",
      "0.90495737 \t 0.16783217 \t 0.11510791 \t 0.64419476 \t 0.458023052741\n",
      "0.9201278 \t 0.0625 \t 0.05128205 \t 0.77563452 \t 0.452386091144\n",
      "0.8859146 \t 0.18723404 \t 0.08510638 \t 0.6746988 \t 0.458238453999\n",
      "0.88498746 \t 0.09722222 \t 0.13903743 \t 0.79787234 \t 0.479779863874\n",
      "0.8779625 \t 0.14334471 \t 0.08130081 \t 0.80635551 \t 0.477240884687\n"
     ]
    }
   ],
   "source": [
    "#'baselines_4/cnn1_ce10_100.h5'\n",
    "print (\"Nh\\t\\t| IH\\t\\t| PH\\t\\t| SH\\t\\t| F-Macro\")\n",
    "print (0.929514,'\\t',0.0000,'\\t\\t',0.0000,'\\t\\t',0.84362934,'\\t', 0.443285836068) \n",
    "#('baselines_4/cnn1_100_focal20_100.h5')\n",
    "print (0.90495737,'\\t',  0.16783217,'\\t',  0.11510791 ,'\\t', 0.64419476,'\\t',0.458023052741)\n",
    "#'baselines_4/cnn1_100_focal20_100_g2.h5'\n",
    "print (0.9201278,'\\t',   0.0625 ,'\\t',     0.05128205 ,'\\t', 0.77563452,'\\t', 0.452386091144)\n",
    "#'baselines_4/cnn1_100_focal20_100_g3.h5\n",
    "print (0.8859146 ,'\\t',  0.18723404  ,'\\t',0.08510638 ,'\\t', 0.6746988,'\\t',0.458238453999)\n",
    "#('baselines_4/rnn1_100_focal15.h5')\n",
    "print (0.88498746 ,'\\t', 0.09722222,'\\t',  0.13903743 ,'\\t', 0.79787234,'\\t',  0.479779863874)\n",
    "#('baselines_4/rnn2_100_focal15.h5')\n",
    "print ( 0.8779625 ,'\\t',  0.14334471,'\\t',  0.08130081  ,'\\t',0.80635551,'\\t',0.477240884687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for keyword argument 'data_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-57eb41a8f8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cnn1_64 = load_model('baselines_4/cnn1_ce10_100.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn1_100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baselines_4/cnn1_100_focal20_100.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcnn1_f2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baselines_4/cnn1_100_focal20_100_g2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn1_f3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baselines_4/cnn1_100_focal20_100_g3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgru1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baselines_4/rnn1_100_focal15.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    315\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    143\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 144\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2512\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2514\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2515\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2500\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2501\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                                            list(custom_objects.items())))\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \"\"\"\n\u001b[0;32m-> 1271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for keyword argument 'data_format'"
     ]
    }
   ],
   "source": [
    "cnn1_64 = load_model('baselines_4/cnn1_ce10_100.h5')\n",
    "cnn1_100 = load_model('baselines_4/cnn1_100_focal20_100.h5')\n",
    "cnn1_f2 = load_model('baselines_4/cnn1_100_focal20_100_g2.h5')\n",
    "cnn1_f3 = load_model('baselines_4/cnn1_100_focal20_100_g3.h5')\n",
    "gru1 = load_model('baselines_4/rnn1_100_focal15.h5')\n",
    "gru2 = load_model('baselines_4/rnn2_100_focal15.h5')\n",
    "\n",
    "\n",
    "list_models=['cnn1_64', 'cnn1_100', 'cnn1_f2', 'cnn1_f3', 'gru1', 'gru2']\n",
    "index_models=np.arange(6)\n",
    "dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "modelos=[cnn1_64, cnn1_100, cnn1_f2, cnn1_f3, gru1, gru2]\n",
    "ind=np.arange(6)\n",
    "dict_trainingModel=dict((key, value) for (key, value) in zip(ind,modelos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_all=[]\n",
    "X_train=np.load(\"matrices/X_train_4.npy\")\n",
    "X_train_rnn=np.load(\"matrices/X_train_rnn_4.npy\")\n",
    "\n",
    "bs=32\n",
    "for i in dict_trainingModel.keys(): \n",
    "    print (\"Agregando predicciones del modelo\", dict_models[i])\n",
    "    a_evaluar=dict_models[i]\n",
    "    if a_evaluar[:3]!='gru':\n",
    "        predicciones_all.append(dict_trainingModel[i].predict(X_train, batch_size=bs))\n",
    "    else:\n",
    "        predicciones_all.append(dict_trainingModel[i].predict(X_train_rnn, batch_size=bs))\n",
    "        \n",
    "predicciones_all_val=[]\n",
    "X_val=np.load(\"matrices/X_val_ig_4.npy\")\n",
    "X_val_rnn=np.load(\"matrices/X_val_rnn_ig_4.npy\")\n",
    "\n",
    "bs=32\n",
    "for i in dict_trainingModel.keys(): \n",
    "    print (\"Agregando predicciones val del modelo\", dict_models[i])\n",
    "    a_evaluar=dict_models[i]\n",
    "    if a_evaluar[:3]!='gru':\n",
    "        predicciones_all_val.append(dict_trainingModel[i].predict(X_val, batch_size=bs))\n",
    "    else:\n",
    "        predicciones_all_val.append(dict_trainingModel[i].predict(X_val_rnn, batch_size=bs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_all_test=[]\n",
    "X_test=np.load(\"\")\n",
    "X_test_rnn=np.load(\"\")\n",
    "\n",
    "bs=32\n",
    "for i in dict_trainingModel.keys(): \n",
    "    print (\"Agregando predicciones test del modelo\", dict_models[i])\n",
    "    a_evaluar=dict_models[i]\n",
    "    if a_evaluar[:3]!='gru':\n",
    "        predicciones_all_test.append(dict_trainingModel[i].predict(X_test, batch_size=bs))\n",
    "    else:\n",
    "        predicciones_all_test.append(dict_trainingModel[i].predict(X_test_rnn, batch_size=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=np.asarray(predicciones_all)\n",
    "print (matrix.shape)\n",
    "matrix_val=np.asarray(predicciones_all_val)\n",
    "print (matrix_val.shape)\n",
    "#matrix_test=np.asarray(predicciones_all_test)\n",
    "#print (matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix=np.concatenate([matrix,matrix_val],axis=1)#,matrix_test], axis=1)\n",
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy_Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, dummy_vectors): \n",
    "        super(Dummy_Embeddings, self).__init__()\n",
    "        aux = torch.from_numpy(np.asarray(dummy_vectors))\n",
    "        self.index2dummy = nn.Embedding(aux.size()[0], d_model)\n",
    "        self.index2dummy.weigth=nn.Parameter(aux)\n",
    "        self.index2dummy.weigth.requires_grad=False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aux=x.numpy()\n",
    "        print (\"aux_size en dummy emb\", aux.shape)\n",
    "        new_x= aux - np.ones(aux.shape)#cambiar aqui! (ver hojita de cuaderno)\n",
    "        new_x= torch.from_numpy(new_x)\n",
    "        return self.index2dummy(new_x.long()) * math.sqrt(self.d_model) #debiese retornar matriz de batch_size x [ind_tw, k1,k2,k3,k4,k5,k6] (si son 6 modelos)\n",
    "    \n",
    "def match(objetos,ejemplo):\n",
    "    i=0\n",
    "    for obj in objetos:\n",
    "        if obj==ejemplo:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "class EncoderDecoderLogSoft(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, sequential):\n",
    "        super(EncoderDecoderLogSoft, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linearSoft = sequential\n",
    "        self.src_embed = src_embed\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out=self.encode(src, src_mask)\n",
    "        retorno=self.toSoftmax(out)\n",
    "        return retorno\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def toSoftmax(self, tensor):\n",
    "        a=tensor.size()[0]\n",
    "        b=tensor.size()[-1]\n",
    "        c=tensor.size()[-2]\n",
    "        new_tensor= np.zeros((a, b))\n",
    "        ini=True\n",
    "        for ai in range(a):\n",
    "            for ci in range(c):\n",
    "                if ini:\n",
    "                    new_tensor[ai]=tensor.data[ai][ci].numpy()\n",
    "                    ini=False\n",
    "                else: \n",
    "                    new_tensor[ai]*=tensor.data[ai][ci].numpy()\n",
    "            ini=True\n",
    "        new_tensor=torch.from_numpy(new_tensor)\n",
    "        lineal=self.linearSoft(new_tensor.float())\n",
    "        return F.log_softmax(lineal, dim = -1)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class EncoderDecoderSoft(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, sequential):\n",
    "        super(EncoderDecoderSoft, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linearSoft = sequential\n",
    "        self.src_embed = src_embed\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out=self.encode(src, src_mask)\n",
    "        retorno=self.toSoftmax(out)\n",
    "        return retorno\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def toSoftmax(self, tensor):\n",
    "        a=tensor.size()[0]\n",
    "        b=tensor.size()[-1]\n",
    "        c=tensor.size()[-2]\n",
    "        new_tensor= np.zeros((a, b))\n",
    "        ini=True\n",
    "        for ai in range(a):\n",
    "            for ci in range(c):\n",
    "                if ini:\n",
    "                    new_tensor[ai]=tensor.data[ai][ci].numpy()\n",
    "                    ini=False\n",
    "                else: \n",
    "                    new_tensor[ai]*=tensor.data[ai][ci].numpy()\n",
    "            ini=True\n",
    "            \n",
    "        new_tensor=torch.from_numpy(new_tensor)\n",
    "        lineal=self.linearSoft(new_tensor.float())\n",
    "        return F.softmax(lineal, dim = -1)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        return  self.encode(src, src_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "    \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "         \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "class PositionalText(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5):\n",
    "        super(PositionalText, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_text=x.data[:,0][:,None,:]\n",
    "        x_machines=x.data[:,1:]\n",
    "        \n",
    "        x_return = x_machines* x_text\n",
    "        return self.dropout(x_return)\n",
    "    \n",
    "class PositionalText_concat(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5):\n",
    "        super(PositionalText_concat, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(batch_size,mode,n_machines,new_matrix):  \n",
    "    \"\"\"ESTA FUNCION DEBERÁ RETORNAR MATRICES DE TAMAÑO BATCH_SIZE * TW,M1,M2,M3,M4,M5,M6\n",
    "    CADA EJEMPLO DEBERÁ SER ESTILO [ID_T, 1,2,1,2,1,2] SIENDO 1 CUANDO LA CLASE PREDICHA ES LA 0\n",
    "    2 PARA CLASE PREDICHA 1, ETC. ID_T DEBEŔA INICIAR EN 5 PARA REFERIRSE AL PRIMER TW (EL PRIMERO DE TRAIN)\"\"\"\n",
    "    x_data=[] \n",
    "    y_data=[]\n",
    "    if mode=='Train':\n",
    "        max_objs=matrix.shape[1]\n",
    "        x_list=np.arange(max_objs)\n",
    "        np.random.shuffle(x_list)\n",
    "        objs=df_train['Unnamed: 0'].values\n",
    "        objs=[obj+5 for obj in objs]\n",
    "        labels=np.load(\"../../forSIMAH/y_train_num_4.npy\")\n",
    "        labels=labels+ np.ones(labels.shape) #(para dejar como espera dummy emb y mask, entre 1 y 4)      \n",
    "    if mode=='Val':\n",
    "        max_objs=matrix_val.shape[1] \n",
    "        x_list=np.arange(max_objs)\n",
    "        x_list=[x+matrix.shape[1] for x in x_list]\n",
    "        np.random.shuffle(x_list)\n",
    "        objs=df_train['Unnamed: 0'].values\n",
    "        objs=[obj+5 for obj in objs]\n",
    "        labels=np.load(\"../../forSIMAH/y_val_num_4.npy\")\n",
    "        labels=labels+ np.ones(labels.shape) #(para dejar como espera dummy emb y mask, entre 1 y 4) \n",
    "    \"\"\"if mode=='Test':\n",
    "        max_objs=matrix_test.shape[1] \n",
    "        x_list=np.arange(max_objs)\n",
    "        x_list=[x+matrix.shape[1]+matrix_val.shape[1] for x in x_list]\n",
    "        np.random.shuffle(x_list)\n",
    "        objs=np.load(\"obj_ID_test.npy\")#np.array(np.load(\"obj_ID_test.npy\")[0])\n",
    "        labels=np.load(\"y_test_models.npy\")#np.array(np.load(\"y_test_models.npy\")[0])\"\"\"\n",
    "        \n",
    "    if len(x_list)%batch_size==0:\n",
    "        n_batches=len(x_list)/batch_size\n",
    "        for nb in range(n_batches): \n",
    "            x_data.append(x_list[nb*batch_size:(nb+1)*batch_size])\n",
    "    else:\n",
    "        n_batches=int(len(x_list)/batch_size)\n",
    "        resto=int(len(x_list)-n_batches*batch_size)\n",
    "        to_repeat=batch_size-resto\n",
    "        \n",
    "        for nb in range(n_batches): \n",
    "            x_data.append(x_list[nb*batch_size:(nb+1)*batch_size])\n",
    "        cola=list(x_list[(nb+1)*batch_size:])\n",
    "        for i in range(to_repeat):\n",
    "            indice= np.random.randint(len(x_list))\n",
    "            cola.append(x_list[indice])\n",
    "        x_data.append(np.asarray(cola))\n",
    "    \n",
    "    new_x_data=[] \n",
    "    if mode=='Train':\n",
    "        for batch in x_data:\n",
    "            temp=[]\n",
    "            for exam in batch:\n",
    "                temp2=[objs[exam]] ### obj evaluado en maquinas\n",
    "                for m in range(n_machines): \n",
    "                    temp2= temp2 + [np.argmax(new_matrix[m][exam])+1]   ###temp2 = [tweetID, index_class]\n",
    "                temp.append(temp2)\n",
    "            new_x_data.append(temp) \n",
    "            \n",
    "    if mode=='Val':\n",
    "        for batch in x_data:\n",
    "            temp=[]\n",
    "            for exam in batch:\n",
    "                temp2=[objs[exam-matrix.shape[1]]]                      ### obj evaluado en maquinas\n",
    "                for m in range(n_machines): \n",
    "                    temp2= temp2 + [np.argmax(new_matrix[m][exam])+1]   ###temp2 = [tweetID, index_class]\n",
    "                temp.append(temp2)\n",
    "            new_x_data.append(temp)\n",
    "            \n",
    "    \"\"\"if mode=='Test':\n",
    "        for batch in x_data:\n",
    "            temp=[]\n",
    "            for exam in batch:\n",
    "                temp2=[objs[exam-matrix.shape[1]-matrix_val.shape[1]]]  ### obj evaluado en maquinas\n",
    "                for m in range(n_machines): \n",
    "                    temp2= temp2 + [np.argmax(new_matrix[m][exam])+1]   ###temp2 = [tweetID, index_class]\n",
    "                temp.append(temp2)\n",
    "            new_x_data.append(temp)\n",
    "    \"\"\"\n",
    "\n",
    "    y_data=[]\n",
    "    for batch in new_x_data:\n",
    "        temp_y=[]\n",
    "        for exam in batch:\n",
    "            q=exam[0]\n",
    "            indice=match(objs, q)    #retorna indice de match tweetID==objs_i\n",
    "            temp_y.append([labels[indice]])       #appendear grounnf truth de tweetID\n",
    "        y_data.append(temp_y)\n",
    "        \n",
    "\n",
    "    new_x_data=np.asarray(new_x_data)\n",
    "    y_data=np.asarray(y_data)\n",
    "    \n",
    "    for x,y in zip(new_x_data, y_data):\n",
    "        src = Variable(torch.from_numpy(np.asarray(x)), requires_grad=False)\n",
    "        stances = Variable(torch.from_numpy(np.asarray(y)), requires_grad=False)\n",
    "        yield Batch(src, stances, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>harassment</th>\n",
       "      <th>IndirectH</th>\n",
       "      <th>PhysicalH</th>\n",
       "      <th>SexualH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>pics mexican school girl naked melanie safka f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT shereiqns Having curly hair is a gotdamn ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BreakingNews 7 Unexplained Prisoners Deaths M...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>girl dirty quarterback latino pokemon young h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>miliondollameat sexting n ngirl daddy n nme l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      tweet_content  harassment  \\\n",
       "0           0  pics mexican school girl naked melanie safka f...           1   \n",
       "1           1  RT shereiqns Having curly hair is a gotdamn ch...           0   \n",
       "2           2   BreakingNews 7 Unexplained Prisoners Deaths M...           0   \n",
       "3           3   girl dirty quarterback latino pokemon young h...           1   \n",
       "4           4   miliondollameat sexting n ngirl daddy n nme l...           1   \n",
       "\n",
       "   IndirectH  PhysicalH  SexualH  \n",
       "0          0          0        1  \n",
       "1          0          0        0  \n",
       "2          0          0        0  \n",
       "3          0          0        1  \n",
       "4          0          0        1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Train_data_compeition.csv\")\n",
    "df_val=pd.read_csv(\"../Validation_data_competition.csv\")\n",
    "df_test=pd.read_csv(\"../testset-competition.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768)\n",
      "(4, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ks=np.load(\"../Orthogonals_768.npy\")\n",
    "print (Ks.shape)\n",
    "random=np.random.randint(Ks.shape[0], size=4)\n",
    "K_list=[Ks[k] for k in random]\n",
    "print (np.asarray(K_list).shape)\n",
    "len(K_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# por qué parte en 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6374,)\n",
      "(2125,)\n",
      "2123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10622"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_list_ext=K_list\n",
    "objs_train=df_train['Unnamed: 0'].values\n",
    "objs_val=df_val['Unnamed: 0'].values\n",
    "objs_test=np.arange(df_test.shape[0])\n",
    "tope=len(objs_train)+len(objs_val)\n",
    "objs_test=[elem+tope for elem in objs_test]\n",
    "print (objs_train.shape)\n",
    "print (objs_val.shape)\n",
    "print (len(objs_test))\n",
    "#print (objs_test.shape)\n",
    "llaves=np.concatenate([objs_train,objs_val,objs_test])\n",
    "#valores=np.arange(5, len(objs_train)+len(objs_val)+len(objs_test)+5)\n",
    "len(llaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_IDObj=dict((key, value) for (key, value) in zip(llaves,valores))\n",
    "dict_ObjID=dict((key, value) for (key, value) in zip(valores,llaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10622"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_ObjID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8499"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ObjID[8504]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2123"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largo de lista de vectores bert (considerando dummy clases) 10626\n"
     ]
    }
   ],
   "source": [
    "contador=0\n",
    "for i in range(tope):\n",
    "    vector=np.load(\"../../vectors/\"+str(i)+\".npy\")\n",
    "    K_list_ext.append(vector)\n",
    "for i in range(len(objs_test)):\n",
    "    vector=np.load(\"../../test_vectors/\"+str(i)+\".npy\")\n",
    "    K_list_ext.append(vector)\n",
    "print (\"largo de lista de vectores bert (considerando dummy clases)\", len(K_list_ext))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10626"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(K_list_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dict_stn=dict()\n",
    "dict_stn[1]=1\n",
    "dict_stn[2]=2\n",
    "dict_stn[3]=3\n",
    "dict_stn[4]=4\n",
    "dict_stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_concat(target, N=2, d_model=768, d_ff=1024, h=4, dropout=0.3, soft=True):  #recibir src_vocab si lo utiliza dummy_Emb\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalText_concat(d_model, dropout)\n",
    "    if soft:\n",
    "        print (\"Creando modelo con salida Softmax\")\n",
    "        model = EncoderDecoderSoft(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Dummy_Embeddings(768, K_list_ext), c(position)),\n",
    "            nn.Linear(d_model, 4))\n",
    "    else:\n",
    "        print (\"\")\n",
    "        print (\"Creando modelo con salida Log_softmax\")\n",
    "        model = EncoderDecoderLogSoft(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Dummy_Embeddings(768, K_list_ext), c(position)),\n",
    "            nn.Linear(d_model, 4))\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :]\n",
    "            self.trg_y = trg[:, 0:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "            \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "    \n",
    "def run_epoch(data_iter, model, loss_compute, mode):\n",
    "    \n",
    "    ## VER ACA \n",
    "    \n",
    "    #print (\"en data iter de run epoch\")\n",
    "    \"\"\"if mode=='Test':\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            total_datos = 0\n",
    "            total_loss = 0\n",
    "            total_acc=0\n",
    "            total_fm1=0\n",
    "            total_fs=0\n",
    "            n_batches = 0\n",
    "            cm_last=np.zeros((4,4))\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                out = model.forward(batch.src, batch.src_mask)\n",
    "                loss,acc,f1,cm = loss_compute(out, batch.trg_y, batch.ntokens, mode)\n",
    "                #la loss del batch es sobre todos los ejemplos o es promedio en batch?\n",
    "                \n",
    "                cm_last+=cm\n",
    "                f_ma=np.mean(f1)\n",
    "                total_loss += loss\n",
    "                #print (\"total_loss\", total_loss)\n",
    "                #print (\"loss del batch\", loss)\n",
    "                #print (\"Acc de batch\",acc)\n",
    "                #print (\"F1 de batch\",f1)\n",
    "                total_acc += np.array(acc)\n",
    "                total_fm1 += np.sum(f1)/4.0\n",
    "                total_fs += f1\n",
    "                total_datos += batch.ntokens\n",
    "                n_batches += 1\n",
    "                #if i % 50 == 0:    \n",
    "                #    print(\"Step: %d Batch Loss: %f Acc: %f F1: %s F1_ma: %f\" % (i,loss/n_batches, acc, str(f1), f_ma))\n",
    "            return total_loss/n_batches, torch.from_numpy(np.array(total_acc))/n_batches, torch.from_numpy(np.array(total_fm1))/n_batches, torch.from_numpy(np.array(total_fs))/n_batches,cm_last\n",
    "    \n",
    "    else:\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    total_learners = 0\n",
    "    total_datos = 0\n",
    "    total_loss = 0\n",
    "    total_acc=0\n",
    "    total_fm1=0\n",
    "    total_fs=0\n",
    "    n_batches = 0\n",
    "    cm_last=np.zeros((4,4))\n",
    "    #print (\"training\")\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.src_mask)\n",
    "        loss,acc,f1,cm = loss_compute(out, batch.trg_y, batch.ntokens, mode)\n",
    "        cm_last+=cm\n",
    "        f_ma=np.mean(f1)\n",
    "        total_loss += loss\n",
    "        total_acc += np.array(acc)\n",
    "        total_fm1 += np.sum(f1)/4.0\n",
    "        total_fs += f1  ###para train\n",
    "        total_datos += batch.ntokens\n",
    "        n_batches += 1\n",
    "    return total_loss/n_batches, torch.from_numpy(np.array(total_acc))/n_batches, torch.from_numpy(np.array(total_fm1))/n_batches, torch.from_numpy(np.array(total_fs))/n_batches, cm_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of learners + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    maximo=max(src_elements, tgt_elements)\n",
    "    return maximo\n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        #print (\"Rate de Noam_opt\", self._rate, self._step)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "    \n",
    "class LabelCCE(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(LabelCCE, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                       \n",
    "    def forward(self, x, target):\n",
    "        return self.criterion(x, target)\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weights, gamma=2.0, reduce=True):#, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.gamma = gamma\n",
    "        #self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        tensors = []\n",
    "        for l in y:\n",
    "            tensors.append(torch.zeros(4).scatter_(0, torch.tensor(l), 1))\n",
    "        result = torch.stack(tensors, 0)\n",
    "        \n",
    "        CCE_loss = F.cross_entropy(x, result.long().argmax(dim=-1), reduction='none', weight=self.weights.float())\n",
    "        pt = torch.exp(-CCE_loss)\n",
    "        factor= (1-pt)**self.gamma * CCE_loss\n",
    "        \n",
    "        F_loss = factor  #*self.weights.float()-- peso esta dentro de la funcion\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "class SimpleLossComputeFL:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, criterion, opt=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm, mode):        \n",
    "        y_new=(y.float()-torch.ones((y.shape[0],y.shape[1]))).int()\n",
    "        ac=accuracy_scorer(x, y_new)\n",
    "        f1=f_scorer(x, y_new)\n",
    "        cm=compute_confusion_matrix(x, y_new)\n",
    "        tempa= x.contiguous().view(-1, x.size(-1))\n",
    "        tempb= y_new.long().contiguous().view(-1)\n",
    "        loss = self.criterion(tempa, tempb) #/ norm\n",
    "        if mode!='Test':\n",
    "            loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return [loss.data, ac ,f1, cm] #*norm #[0] * norm\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESDE ACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_scorer(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return accuracy_score(target.numpy(),predicho.numpy())\n",
    "\n",
    "def f_scorer(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return f1_score(target.numpy(),predicho.numpy(), average=None, labels=[0,1,2,3])\n",
    "\n",
    "def compute_confusion_matrix(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return confusion_matrix(target.numpy(), predicho.numpy(), labels=[0,1,2,3])\n",
    "\n",
    "y_train_stance=np.load(\"../Numpy_matrix/y_train_models.npy\")\n",
    "print (y_train_stance[:10])\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_stance), y_train_stance)#{0: 3.,   1: 6.,   2: 5.,  3: 3.}\n",
    "print (class_weights)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
