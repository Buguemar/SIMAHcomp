{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import os, re, sys\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "import ast\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "'''\n",
    "Compatible with tensorflow backend\n",
    "gamma entre más alto más tolerante (queremos un gamma chico pero no demasiado! 0.25 - 5)\n",
    "alpha pesos por clase (weights)\n",
    "'''\n",
    "def focal_loss(gamma=2., weights=1):   #weights np.asarray()\n",
    "    weights= K.variable(weights)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.clip(y_true, K.epsilon(),1)\n",
    "        y_pred = K.clip(y_pred,K.epsilon(),1)\n",
    "        return - K.sum(weights* K.pow(1. - y_pred, gamma)* y_true * K.log(y_pred), axis=-1) \n",
    "    return focal_loss_fixed\n",
    "\n",
    "keras.losses.focal_loss=keras.losses.MSE\n",
    "keras.losses.focal_loss_fixed=keras.losses.MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nh\t\t| IH\t\t| PH\t\t| SH\t\t| F-Macro\n",
      "0.929514 \t 0.0 \t\t 0.0 \t\t 0.84362934 \t 0.443285836068\n",
      "0.90495737 \t 0.16783217 \t 0.11510791 \t 0.64419476 \t 0.458023052741\n",
      "0.9201278 \t 0.0625 \t 0.05128205 \t 0.77563452 \t 0.452386091144\n",
      "0.8859146 \t 0.18723404 \t 0.08510638 \t 0.6746988 \t 0.458238453999\n",
      "0.88498746 \t 0.09722222 \t 0.13903743 \t 0.79787234 \t 0.479779863874\n",
      "0.8779625 \t 0.14334471 \t 0.08130081 \t 0.80635551 \t 0.477240884687\n"
     ]
    }
   ],
   "source": [
    "#'baselines_4/cnn1_ce10_100.h5'\n",
    "print (\"Nh\\t\\t| IH\\t\\t| PH\\t\\t| SH\\t\\t| F-Macro\")\n",
    "print (0.929514,'\\t',0.0000,'\\t\\t',0.0000,'\\t\\t',0.84362934,'\\t', 0.443285836068) \n",
    "#('baselines_4/cnn1_100_focal20_100.h5')\n",
    "print (0.90495737,'\\t',  0.16783217,'\\t',  0.11510791 ,'\\t', 0.64419476,'\\t',0.458023052741)\n",
    "#'baselines_4/cnn1_100_focal20_100_g2.h5'\n",
    "print (0.9201278,'\\t',   0.0625 ,'\\t',     0.05128205 ,'\\t', 0.77563452,'\\t', 0.452386091144)\n",
    "#'baselines_4/cnn1_100_focal20_100_g3.h5\n",
    "print (0.8859146 ,'\\t',  0.18723404  ,'\\t',0.08510638 ,'\\t', 0.6746988,'\\t',0.458238453999)\n",
    "#('baselines_4/rnn1_100_focal15.h5')\n",
    "print (0.88498746 ,'\\t', 0.09722222,'\\t',  0.13903743 ,'\\t', 0.79787234,'\\t',  0.479779863874)\n",
    "#('baselines_4/rnn2_100_focal15.h5')\n",
    "print ( 0.8779625 ,'\\t',  0.14334471,'\\t',  0.08130081  ,'\\t',0.80635551,'\\t',0.477240884687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn1_64 = load_model('baselines_4/cnn1_ce10_100.h5')\n",
    "cnn1_100 = load_model('baselines_4/cnn1_100_focal20_100.h5')\n",
    "cnn1_f2 = load_model('baselines_4/cnn1_100_focal20_100_g2.h5')\n",
    "cnn1_f3 = load_model('baselines_4/cnn1_100_focal20_100_g3.h5')\n",
    "gru1 = load_model('baselines_4/rnn1_100_focal15.h5')\n",
    "gru2 = load_model('baselines_4/rnn2_100_focal15.h5')\n",
    "\n",
    "\n",
    "list_models=['cnn1_64', 'cnn1_100', 'cnn1_f2', 'cnn1_f3', 'gru1', 'gru2']\n",
    "index_models=np.arange(6)\n",
    "dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "modelos=[cnn1_64, cnn1_100, cnn1_f2, cnn1_f3, gru1, gru2]\n",
    "ind=np.arange(6)\n",
    "dict_trainingModel=dict((key, value) for (key, value) in zip(ind,modelos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando predicciones del modelo cnn1_64\n",
      "Agregando predicciones del modelo cnn1_100\n",
      "Agregando predicciones del modelo cnn1_f2\n",
      "Agregando predicciones del modelo cnn1_f3\n",
      "Agregando predicciones del modelo gru1\n",
      "Agregando predicciones del modelo gru2\n",
      "Agregando predicciones val del modelo cnn1_64\n",
      "Agregando predicciones val del modelo cnn1_100\n",
      "Agregando predicciones val del modelo cnn1_f2\n",
      "Agregando predicciones val del modelo cnn1_f3\n",
      "Agregando predicciones val del modelo gru1\n",
      "Agregando predicciones val del modelo gru2\n"
     ]
    }
   ],
   "source": [
    "predicciones_all=[]\n",
    "X_train=np.load(\"matrices/X_train_4.npy\")\n",
    "X_train_rnn=np.load(\"matrices/X_train_rnn_4.npy\")\n",
    "\n",
    "bs=32\n",
    "for i in dict_trainingModel.keys(): \n",
    "    print (\"Agregando predicciones del modelo\", dict_models[i])\n",
    "    a_evaluar=dict_models[i]\n",
    "    if a_evaluar[:3]!='gru':\n",
    "        predicciones_all.append(dict_trainingModel[i].predict(X_train, batch_size=bs))\n",
    "    else:\n",
    "        predicciones_all.append(dict_trainingModel[i].predict(X_train_rnn, batch_size=bs))\n",
    "        \n",
    "predicciones_all_val=[]\n",
    "X_val=np.load(\"matrices/X_val_ig_4.npy\")\n",
    "X_val_rnn=np.load(\"matrices/X_val_rnn_ig_4.npy\")\n",
    "\n",
    "bs=32\n",
    "for i in dict_trainingModel.keys(): \n",
    "    print (\"Agregando predicciones val del modelo\", dict_models[i])\n",
    "    a_evaluar=dict_models[i]\n",
    "    if a_evaluar[:3]!='gru':\n",
    "        predicciones_all_val.append(dict_trainingModel[i].predict(X_val, batch_size=bs))\n",
    "    else:\n",
    "        predicciones_all_val.append(dict_trainingModel[i].predict(X_val_rnn, batch_size=bs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicciones_all_test=[]\n",
    "X_test=np.load(\"\")\n",
    "X_test_rnn=np.load(\"\")\n",
    "\n",
    "bs=32\n",
    "for i in dict_trainingModel.keys(): \n",
    "    print (\"Agregando predicciones test del modelo\", dict_models[i])\n",
    "    a_evaluar=dict_models[i]\n",
    "    if a_evaluar[:3]!='gru':\n",
    "        predicciones_all_test.append(dict_trainingModel[i].predict(X_test, batch_size=bs))\n",
    "    else:\n",
    "        predicciones_all_test.append(dict_trainingModel[i].predict(X_test_rnn, batch_size=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6374, 4)\n",
      "(6, 2125, 4)\n"
     ]
    }
   ],
   "source": [
    "matrix=np.asarray(predicciones_all)\n",
    "print (matrix.shape)\n",
    "matrix_val=np.asarray(predicciones_all_val)\n",
    "print (matrix_val.shape)\n",
    "#matrix_test=np.asarray(predicciones_all_test)\n",
    "#print (matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8499, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix=np.concatenate([matrix,matrix_val],axis=1)#,matrix_test], axis=1)\n",
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768)\n",
      "(4, 768)\n"
     ]
    }
   ],
   "source": [
    "Ks=np.load(\"../Orthogonals_768.npy\")\n",
    "print (Ks.shape)\n",
    "random=np.random.randint(Ks.shape[0], size=4)\n",
    "K_list=[Ks[k] for k in random]\n",
    "print (np.asarray(K_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dummy_Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, dummy_vectors, dict_ind_emb): \n",
    "        super(Dummy_Embeddings, self).__init__()\n",
    "        aux = torch.from_numpy(np.asarray(dummy_vectors))\n",
    "        self.index2dummy = nn.Embedding(aux.size()[0], d_model)\n",
    "        self.index2dummy.weigth=nn.Parameter(aux)\n",
    "        self.index2dummy.weigth.requires_grad=False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aux=x.data\n",
    "        max_a=aux.size()[0]\n",
    "        max_b=aux.size()[1]\n",
    "        new_x= np.zeros((max_a,max_b),dtype='int32')\n",
    "        for obj in range(max_a):\n",
    "            for elem in range(max_b):\n",
    "                #print (\"en dummy embedding x.data[obj][elem]\",x.data[obj][elem].item())\n",
    "                new_x[obj,elem] = dict_ind_emb[x.data[obj][elem].item()] - 1\n",
    "        return self.index2dummy(torch.from_numpy(new_x).long()) * math.sqrt(self.d_model)\n",
    "    \n",
    "def match(objetos,ejemplo):\n",
    "    i=0\n",
    "    for obj in objetos:\n",
    "        if obj==ejemplo:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "class EncoderDecoderLogSoft(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, sequential):\n",
    "        super(EncoderDecoderLogSoft, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linearSoft = sequential\n",
    "        self.src_embed = src_embed\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out=self.encode(src, src_mask)\n",
    "        retorno=self.toSoftmax(out)\n",
    "        return retorno\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def toSoftmax(self, tensor):\n",
    "        a=tensor.size()[0]\n",
    "        b=tensor.size()[-1]\n",
    "        c=tensor.size()[-2]\n",
    "        new_tensor= np.zeros((a, b))\n",
    "        ini=True\n",
    "        for ai in range(a):\n",
    "            for ci in range(c):\n",
    "                if ini:\n",
    "                    new_tensor[ai]=tensor.data[ai][ci].numpy()\n",
    "                    ini=False\n",
    "                else: \n",
    "                    new_tensor[ai]*=tensor.data[ai][ci].numpy()\n",
    "            ini=True\n",
    "        new_tensor=torch.from_numpy(new_tensor)\n",
    "        lineal=self.linearSoft(new_tensor.float())\n",
    "        return F.log_softmax(lineal, dim = -1)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class EncoderDecoderSoft(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, sequential):\n",
    "        super(EncoderDecoderSoft, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linearSoft = sequential\n",
    "        self.src_embed = src_embed\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out=self.encode(src, src_mask)\n",
    "        retorno=self.toSoftmax(out)\n",
    "        return retorno\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def toSoftmax(self, tensor):\n",
    "        a=tensor.size()[0]\n",
    "        b=tensor.size()[-1]\n",
    "        c=tensor.size()[-2]\n",
    "        new_tensor= np.zeros((a, b))\n",
    "        ini=True\n",
    "        for ai in range(a):\n",
    "            for ci in range(c):\n",
    "                if ini:\n",
    "                    new_tensor[ai]=tensor.data[ai][ci].numpy()\n",
    "                    ini=False\n",
    "                else: \n",
    "                    new_tensor[ai]*=tensor.data[ai][ci].numpy()\n",
    "            ini=True\n",
    "            \n",
    "        new_tensor=torch.from_numpy(new_tensor)\n",
    "        lineal=self.linearSoft(new_tensor.float())\n",
    "        return F.softmax(lineal, dim = -1)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        return  self.encode(src, src_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "    \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "         \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "class PositionalText(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5):\n",
    "        super(PositionalText, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_text=x.data[:,0][:,None,:]\n",
    "        x_machines=x.data[:,1:]\n",
    "        \n",
    "        x_return = x_machines* x_text\n",
    "        return self.dropout(x_return)\n",
    "    \n",
    "class PositionalText_concat(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5):\n",
    "        super(PositionalText_concat, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_gen(batch_size,mode,n_machines,new_matrix):    \n",
    "    x_data=[] \n",
    "    y_data=[]\n",
    "    if mode=='Train':\n",
    "        max_objs=matrix.shape[1]\n",
    "        x_list=np.arange(max_objs)\n",
    "        np.random.shuffle(x_list)\n",
    "        objs=np.load(\"obj_ID_train.npy\")\n",
    "        labels=np.load(\"../Numpy_matrix/y_train_models.npy\")        \n",
    "    if mode=='Val':\n",
    "        max_objs=matrix_val.shape[1] \n",
    "        x_list=np.arange(max_objs)\n",
    "        x_list=[x+matrix.shape[1] for x in x_list]\n",
    "        np.random.shuffle(x_list)\n",
    "        objs=np.load(\"obj_ID_val.npy\")\n",
    "        labels=np.load(\"../Numpy_matrix/y_val_models.npy\")\n",
    "    if mode=='Test':\n",
    "        max_objs=matrix_test.shape[1] \n",
    "        x_list=np.arange(max_objs)\n",
    "        x_list=[x+matrix.shape[1]+matrix_val.shape[1] for x in x_list]\n",
    "        np.random.shuffle(x_list)\n",
    "        objs=np.load(\"obj_ID_test.npy\")#np.array(np.load(\"obj_ID_test.npy\")[0])\n",
    "        labels=np.load(\"y_test_models.npy\")#np.array(np.load(\"y_test_models.npy\")[0])\n",
    "        \n",
    "    if len(x_list)%batch_size==0:\n",
    "        n_batches=len(x_list)/batch_size\n",
    "        for nb in range(n_batches): \n",
    "            x_data.append(x_list[nb*batch_size:(nb+1)*batch_size])\n",
    "    else:\n",
    "        n_batches=int(len(x_list)/batch_size)\n",
    "        for nb in range(n_batches): \n",
    "            x_data.append(x_list[nb*batch_size:(nb+1)*batch_size])\n",
    "    \n",
    "    new_x_data=[] \n",
    "    if mode=='Train':\n",
    "        for batch in x_data:\n",
    "            temp=[]\n",
    "            for exam in batch:\n",
    "                temp2=[objs[exam]] ### obj evaluado en maquinas\n",
    "                for m in range(n_machines): \n",
    "                    temp2= temp2 + [np.argmax(new_matrix[m][exam])+1]   ###temp2 = [tweetID, index_class]\n",
    "                temp.append(temp2)\n",
    "            new_x_data.append(temp) \n",
    "            \n",
    "    if mode=='Val':\n",
    "        for batch in x_data:\n",
    "            temp=[]\n",
    "            for exam in batch:\n",
    "                temp2=[objs[exam-matrix.shape[1]]]                      ### obj evaluado en maquinas\n",
    "                for m in range(n_machines): \n",
    "                    temp2= temp2 + [np.argmax(new_matrix[m][exam])+1]   ###temp2 = [tweetID, index_class]\n",
    "                temp.append(temp2)\n",
    "            new_x_data.append(temp)\n",
    "            \n",
    "    if mode=='Test':\n",
    "        for batch in x_data:\n",
    "            temp=[]\n",
    "            for exam in batch:\n",
    "                temp2=[objs[exam-matrix.shape[1]-matrix_val.shape[1]]]  ### obj evaluado en maquinas\n",
    "                for m in range(n_machines): \n",
    "                    temp2= temp2 + [np.argmax(new_matrix[m][exam])+1]   ###temp2 = [tweetID, index_class]\n",
    "                temp.append(temp2)\n",
    "            new_x_data.append(temp)\n",
    "\n",
    "    y_data=[]\n",
    "    for batch in new_x_data:\n",
    "        temp_y=[]\n",
    "        for exam in batch:\n",
    "            q=exam[0]\n",
    "            indice=match(objs, q)    #retorna indice de match tweetID==objs_i\n",
    "            temp_y.append([labels[indice]+1])       #appendear grounnf truth de tweetID\n",
    "        y_data.append(temp_y)\n",
    "        \n",
    "\n",
    "    new_x_data=np.asarray(new_x_data)\n",
    "    y_data=np.asarray(y_data)\n",
    "    \n",
    "    for x,y in zip(new_x_data, y_data):\n",
    "        src = Variable(torch.from_numpy(np.asarray(x)), requires_grad=False)\n",
    "        stances = Variable(torch.from_numpy(np.asarray(y)), requires_grad=False)\n",
    "        yield Batch(src, stances, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K_list_ext=K_list\n",
    "objs_train=np.load(\"obj_ID_train.npy\")\n",
    "objs_val=np.load(\"obj_ID_val.npy\")\n",
    "objs_test=np.load(\"obj_ID_test.npy\")\n",
    "print (objs_train.shape)\n",
    "print (objs_val.shape)\n",
    "print (objs_test.shape)\n",
    "llaves=np.concatenate([objs_train,objs_val,objs_test])\n",
    "valores=np.arange(5, objs_train.shape[0]+objs_val.shape[0]+objs_test.shape[0]+5)\n",
    "len(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_IDObj=dict((key, value) for (key, value) in zip(llaves,valores))\n",
    "dict_ObjID=dict((key, value) for (key, value) in zip(valores,llaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dict_ObjID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (len(K_list_ext))\n",
    "for indice in dict_ObjID.keys():\n",
    "    try:\n",
    "        vector=np.load(\"NumpyVectors/\"+str(dict_ObjID[indice])+\".npy\")\n",
    "        K_list_ext.append(vector)\n",
    "        print (\"Cargando desde Train-Val\")\n",
    "    except:\n",
    "        vector=np.load(\"../NumpyTest/\"+str(dict_ObjID[indice])+\".npy\")\n",
    "        K_list_ext.append(vector)\n",
    "        print (\"Cargando desde Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(K_list_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_stn=dict()\n",
    "dict_stn[1]=1\n",
    "dict_stn[2]=2\n",
    "dict_stn[3]=3\n",
    "dict_stn[4]=4\n",
    "dict_stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from itertools import chain\n",
    "\n",
    "#dict_ind_emb=dict(chain.from_iterable(d.iteritems() for d in (dict_stn, dict_IDObj)))\n",
    "dict_ind_emb = {**dict_stn, **dict_IDObj}\n",
    "dict_ind_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(target, N=2, d_model=768, d_ff=1024, h=4, dropout=0.3):  #recibir src_vocab si lo utiliza dummy_Emb\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalText(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Dummy_Embeddings(768, K_list_ext, dict_ind_emb), c(position)),\n",
    "        Generator(d_model, target))\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model\n",
    "\n",
    "def make_model_concat(target, N=2, d_model=768, d_ff=1024, h=4, dropout=0.3, tipo='Soft'):  #recibir src_vocab si lo utiliza dummy_Emb\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalText_concat(d_model, dropout)\n",
    "    if tipo=='log':\n",
    "        print (\"\")\n",
    "        print (\"Creando modelo con salida Log_softmax\")\n",
    "        model = EncoderDecoderLogSoft(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Dummy_Embeddings(768, K_list_ext, dict_ind_emb), c(position)),\n",
    "        nn.Linear(d_model, 4))\n",
    "    else:\n",
    "        print (\"Creando modelo con salida Softmax\")\n",
    "        model = EncoderDecoderSoft(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Dummy_Embeddings(768, K_list_ext, dict_ind_emb), c(position)),\n",
    "            nn.Linear(d_model, 4))\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :]\n",
    "            self.trg_y = trg[:, 0:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "            \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "    \n",
    "def run_epoch(data_iter, model, loss_compute, mode):\n",
    "    #print (\"en data iter de run epoch\")\n",
    "    if mode=='Test':\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            total_datos = 0\n",
    "            total_loss = 0\n",
    "            total_acc=0\n",
    "            total_fm1=0\n",
    "            total_fs=0\n",
    "            n_batches = 0\n",
    "            cm_last=np.zeros((4,4))\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                out = model.forward(batch.src, batch.src_mask)\n",
    "                loss,acc,f1,cm = loss_compute(out, batch.trg_y, batch.ntokens, mode)\n",
    "                #la loss del batch es sobre todos los ejemplos o es promedio en batch?\n",
    "                \n",
    "                cm_last+=cm\n",
    "                f_ma=np.mean(f1)\n",
    "                total_loss += loss\n",
    "                \"\"\"print (\"total_loss\", total_loss)\n",
    "                print (\"loss del batch\", loss)\n",
    "                print (\"Acc de batch\",acc)\n",
    "                print (\"F1 de batch\",f1)\"\"\"\n",
    "                total_acc += np.array(acc)\n",
    "                total_fm1 += np.sum(f1)/4.0\n",
    "                total_fs += f1\n",
    "                total_datos += batch.ntokens\n",
    "                n_batches += 1\n",
    "                #if i % 50 == 0:    \n",
    "                #    print(\"Step: %d Batch Loss: %f Acc: %f F1: %s F1_ma: %f\" % (i,loss/n_batches, acc, str(f1), f_ma))\n",
    "            return total_loss/n_batches, torch.from_numpy(np.array(total_acc))/n_batches, torch.from_numpy(np.array(total_fm1))/n_batches, torch.from_numpy(np.array(total_fs))/n_batches,cm_last\n",
    "    else:\n",
    "        start = time.time()\n",
    "        total_learners = 0\n",
    "        total_datos = 0\n",
    "        total_loss = 0\n",
    "        total_acc=0\n",
    "        total_fm1=0\n",
    "        total_fs=0\n",
    "        n_batches = 0\n",
    "        cm_last=np.zeros((4,4))\n",
    "        #print (\"training\")\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            out = model.forward(batch.src, batch.src_mask)\n",
    "            loss,acc,f1,cm = loss_compute(out, batch.trg_y, batch.ntokens, mode)\n",
    "            cm_last+=cm\n",
    "            f_ma=np.mean(f1)\n",
    "            total_loss += loss\n",
    "            total_acc += np.array(acc)\n",
    "            total_fm1 += np.sum(f1)/4.0\n",
    "            total_fs += f1  ###para train\n",
    "            total_datos += batch.ntokens\n",
    "            n_batches += 1\n",
    "            #if i % 1 == 0:    ##### cambiar a 50 de acuerdo a mis datos. \n",
    "            #    print(\"Epoch Step: %d Loss: %f Acc: %f F1: %s F1_ma: %f\" % (i,loss/n_batches, acc, str(f1), f_ma))\n",
    "        return total_loss/n_batches, torch.from_numpy(np.array(total_acc))/n_batches, torch.from_numpy(np.array(total_fm1))/n_batches, torch.from_numpy(np.array(total_fs))/n_batches, cm_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of learners + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    maximo=max(src_elements, tgt_elements)\n",
    "    return maximo\n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        #print (\"Rate de Noam_opt\", self._rate, self._step)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "    \n",
    "class LabelCCE(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(LabelCCE, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                       \n",
    "    def forward(self, x, target):\n",
    "        return self.criterion(x, target)\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weights, gamma=2.0, reduce=True):#, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.gamma = gamma\n",
    "        #self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        tensors = []\n",
    "        for l in y:\n",
    "            tensors.append(torch.zeros(4).scatter_(0, torch.tensor(l), 1))\n",
    "        result = torch.stack(tensors, 0)\n",
    "        \n",
    "        CCE_loss = F.cross_entropy(x, result.long().argmax(dim=-1), reduction='none', weight=self.weights.float())\n",
    "        pt = torch.exp(-CCE_loss)\n",
    "        factor= (1-pt)**self.gamma * CCE_loss\n",
    "        \n",
    "        F_loss = factor  #*self.weights.float()-- peso esta dentro de la funcion\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "class SimpleLossComputeFL:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, criterion, opt=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm, mode):        \n",
    "        y_new=(y.float()-torch.ones((y.shape[0],y.shape[1]))).int()\n",
    "        ac=accuracy_scorer(x, y_new)\n",
    "        f1=f_scorer(x, y_new)\n",
    "        cm=compute_confusion_matrix(x, y_new)\n",
    "        tempa= x.contiguous().view(-1, x.size(-1))\n",
    "        tempb= y_new.long().contiguous().view(-1)\n",
    "        loss = self.criterion(tempa, tempb) #/ norm\n",
    "        if mode!='Test':\n",
    "            loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return [loss.data, ac ,f1, cm] #*norm #[0] * norm\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_scorer(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return accuracy_score(target.numpy(),predicho.numpy())\n",
    "\n",
    "def f_scorer(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return f1_score(target.numpy(),predicho.numpy(), average=None, labels=[0,1,2,3])\n",
    "\n",
    "def compute_confusion_matrix(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return confusion_matrix(target.numpy(), predicho.numpy(), labels=[0,1,2,3])\n",
    "\n",
    "y_train_stance=np.load(\"../Numpy_matrix/y_train_models.npy\")\n",
    "print (y_train_stance[:10])\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_stance), y_train_stance)#{0: 3.,   1: 6.,   2: 5.,  3: 3.}\n",
    "print (class_weights)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
